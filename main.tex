\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{}
\usepackage[T1]{fontenc}
\title{Révision français}
\author{Louis Hardy}
\date{Mai 2019}

\renewcommand{\baselinestretch}{0.2}

\setlength{\hoffset}{-18pt}         
\setlength{\oddsidemargin}{0pt} % Marge gauche sur pages impaires
\setlength{\evensidemargin}{0pt} % Marge gauche sur pages paires
\setlength{\marginparwidth}{54pt} % Largeur de note dans la marge
\setlength{\textwidth}{481pt} % Largeur de la zone de texte (17cm)
\setlength{\voffset}{-18pt} % Bon pour DOS
\setlength{\marginparsep}{7pt} % Séparation de la marge
\setlength{\topmargin}{0pt} % Pas de marge en haut
\setlength{\headheight}{13pt} % Haut de page
\setlength{\headsep}{4pt} % Entre le haut de page et le texte
\setlength{\footskip}{27pt} % Bas de page + séparation
\setlength{\textheight}{720pt} % Hauteur de la zone de texte (25cm)

\usepackage{natbib}
\usepackage{graphicx}

\begin{document}

\section*{Springboard - Blog}
https://www.springboard.com/blog/machine-learning-interview-questions/ \\
\begin{enumerate}
   \item Bias vs Variance
   \begin{itemize}
     \item \textbf{Bias} is due to erroneus or overly simplistic assumptions in learning algorithm
     \item Usually underfitting your data
     \item \textbf{Variance} typically due to too much complexity in learning algorithm
     \item Makes the model sensitive to high degrees of variation in training data. 
     \item Too much noise from training data
     \item If you make data more complex and add more variables, you'll lose bias but gain variance. 
   \end{itemize}
   \item Supervised vs Unsupervised learning
   \begin{itemize}
       \item Supervised requires labeled data. Unsupervised does not. 
    \end{itemize}
    \item How is KNN different from k-means clustering?
    \begin{itemize}
        \item KNN is a supervised classification algorithm. 
        \item K-means clustering is unsupervised. 
        \item Works very similarly
        \item KNN required labelled data 
        \item K means clustering requires only a set of unlabeled point and a threshold
        \item The algorithm will gradually \textit{learn} how to cluster them by computing mean of the 
              distance between different points.
    \end{itemize}
    \item How does a ROC curve work
    \begin{itemize}
      \item graphical representation of constrast between true and false positive rate at various 
            threholds. 
      \item Used as a proxy for trade-off between sensitivity of model (true positive) vs the 
            fall-out or probability it will trigger a false alarm (false positives)
      \item Think about recall and precision in this case.
        \begin{itemize}
          \item \textit{ex. } You'd have perfect recall (there are actually 10 apples, and you 
                predicted there would be 10) but $66.7\%$ precision because out of the 15 events 
                you precited, only 10 (the apples) are correct.
        \end{itemize}
    \end{itemize}
    \item  Baye's Theorem?
    \begin{itemize}
      \item $$P(A|B) = \frac{P(B|A)P(A)}{P(B)}$$
      \item Leads to a branch of ML called Naive Bayes classifier
    \end{itemize}
    \item Why is "Naive" Bayes naive?
    \begin{itemize}
      \item Used a lot in text mining
      \item It's naive because it makes an assumption that is virtually impossible in real-life data. 
      \begin{itemize}
        \item conditional probability is calculated as the pure product of the individial probabilities of components.
        \item This implies absolute independence of features - condition probably never met in real life.
      \end{itemize}
      \item Anohter way put, if a Naive Bayes classifier figured that you liked pickles and ice-cream 
            would probably naively recommend you a pickle ice-cream.
    \end{itemize}
    \item Difference between L1 and L2 regularization
    \begin{itemize}
      \item Regularization helps solve over-fitting problems in ML
      \item Simple model will be very poor generalization of data. 
      \item Complex model may not perform well in test due to over-fitting.
      \item Regulatization refers to adding a penalty term to objective function and control model complexity using 
            that penalty term. 
      \item Ridge regression used $L_2$ norm for regularization.
      \begin{itemize}
        \item 
      \end{itemize}
    \end{itemize}
\end{enumerate}
\noindent\hrulefill

\end{document}